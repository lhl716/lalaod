import torch
import clip
import os
import cv2
from PIL import Image
import numpy as np
import joblib
from collections import defaultdict
from typing import Dict, List, Tuple
from datetime import datetime
from urllib.request import urlretrieve
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator

class Knowledge_Base:
    def __init__(self, device: str = None, clip_model: str = 'ViT-B/32', sam_model_type: str = "vit_h"):
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.model, self.preprocess = clip.load(clip_model, self.device)
        self.knowledge = defaultdict(lambda: {
            "text_embeds": None,
            "image_embeds": defaultdict(list)
        })
        self.text_cache = {}
        self.index_cache = {}
        
        self.sam = self._init_sam(sam_model_type)
        self.mask_generator = SamAutomaticMaskGenerator(
            model=self.sam,
            points_per_side=64,
            pred_iou_thresh=0.92,
            stability_score_thresh=0.95,
            crop_n_layers=2,
            crop_n_points_downscale_factor=2,
            min_mask_region_area=100  # è¿‡æ»¤å°åŒºåŸŸ
        )
        self.similarity_threshold = 0.2
    
    def _init_sam(self, model_type: str):
        """è‡ªåŠ¨æ£€æµ‹å¹¶ä¸‹è½½SAMæƒé‡çš„åˆå§‹åŒ–æ–¹æ³•"""
        # æ¨¡å‹é…ç½®
        model_config = {
            "vit_h": {
                "url": "https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth",
                "md5": "4b8939a88964f0f4ff5f5b2642c598a6",
                "path": "./weights/sam_vit_h_4b8939.pth"
            },
            "vit_l": {
                "url": "https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth",
                "md5": "0b3195f0f5eb5f2a102916e6185a4e3a",
                "path": "./weights/sam_vit_l_0b3195.pth"
            },
            "vit_b": {
                "url": "https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth",
                "md5": "01ec64d29a2fca3f334193cf5b535a03",
                "path": "./weights/sam_vit_b_01ec64.pth"
            }
        }

        # éªŒè¯æ¨¡å‹ç±»å‹
        if model_type not in model_config:
            raise ValueError(f"Invalid model_type: {model_type}. Choose from {list(model_config.keys())}")

        cfg = model_config[model_type]
        os.makedirs(os.path.dirname(cfg["path"]), exist_ok=True)

        # ä¸‹è½½æ£€æŸ¥ï¼ˆå¸¦MD5æ ¡éªŒï¼‰
        if not self._check_file(cfg["path"], cfg["md5"]):
            print(f"Downloading {model_type} model...")
            self._download_with_progress(cfg["url"], cfg["path"])
            
            if not self._check_file(cfg["path"], cfg["md5"]):
                raise RuntimeError(f"Failed to download valid {model_type} model. "
                                  "Please download manually from: {cfg['url']}")

        # åŠ è½½æ¨¡å‹
        sam = sam_model_registry[model_type](checkpoint=cfg["path"])
        sam.to(device=self.device)
        return sam

    def _check_file(self, path: str, expected_md5: str) -> bool:
        """æ£€æŸ¥æ–‡ä»¶å®Œæ•´æ€§å’ŒMD5"""
        if not os.path.exists(path):
            return False
        
        try:
            import hashlib
            with open(path, "rb") as f:
                md5 = hashlib.md5(f.read()).hexdigest()
            return md5 == expected_md5
        except Exception:
            return False

    def _download_with_progress(self, url: str, filename: str):
        """å¸¦è¿›åº¦æ¡çš„ä¸‹è½½æ–¹æ³•"""
        try:
            from tqdm import tqdm
            class DownloadProgressBar(tqdm):
                def update_to(self, b=1, bsize=1, tsize=None):
                    if tsize is not None:
                        self.total = tsize
                    self.update(b * bsize - self.n)

            with DownloadProgressBar(unit='B', unit_scale=True, miniters=1, desc=os.path.basename(filename)) as t:
                urlretrieve(url, filename=filename, reporthook=t.update_to)
        except ImportError:
            print("tqdm not installed, downloading without progress bar...")
            urlretrieve(url, filename=filename)
        
    def _parse_attribute_text(self, text: str) -> Tuple[str, List[str]]:
        """è§£æå±æ€§æ–‡æœ¬ä¸ºç±»åå’Œå±æ€§åˆ—è¡¨"""
        class_name, attributes = text.split(":", 1)
        class_name = class_name.strip()
        attributes = [class_name] + [attr.strip() for attr in attributes.strip(" .").split(", ")]
        return class_name, attributes

    def _encode_texts(self, texts: List[str]) -> torch.Tensor:
        """æ‰¹é‡æ–‡æœ¬ç¼–ç """
        with torch.no_grad():
            text_inputs = clip.tokenize(texts).to(self.device)
            return self.model.encode_text(text_inputs)
    
    def process_attributes(self, attribute_text: str) -> Dict[str, torch.Tensor]:
        """å¤„ç†å±æ€§æ–‡æœ¬ç”ŸæˆåµŒå…¥ï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        class_name, components = self._parse_attribute_text(attribute_text)
        embeddings = self._encode_texts(components)
        
        # åˆ†è§£ä¸ºç‹¬ç«‹çš„å¼ é‡
        self.text_cache[class_name] = {
            "overall": embeddings[0],
            "class": embeddings[1],
            "attributes": [embeddings[i] for i in range(2, len(components))],  # æ”¹ä¸ºåˆ—è¡¨å­˜å‚¨
            "attribute_names": components[2:]
        }
        return self.text_cache[class_name]
    
    def _segment_image(self, image: Image.Image) -> List[Image.Image]:
        """ä½¿ç”¨SAMè¿›è¡Œé«˜è´¨é‡å›¾åƒåˆ†å‰²"""
        # è½¬æ¢å›¾åƒæ ¼å¼
        cv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
        original_size = cv_image.shape[:2]
        
        # è°ƒæ•´å›¾åƒå°ºå¯¸ï¼ˆä¿æŒé•¿è¾¹ä¸è¶…è¿‡1024ï¼‰
        scale_factor = max(1024 / max(original_size), 1.0)
        new_size = (int(original_size[1] * scale_factor), 
                   int(original_size[0] * scale_factor))
        resized_image = cv2.resize(cv_image, new_size, interpolation=cv2.INTER_LINEAR)
        
        # ç”Ÿæˆæ©ç 
        with torch.autocast(device_type="cuda" if "cuda" in self.device else "cpu"):
            masks = self.mask_generator.generate(resized_image)
        
        # è¿‡æ»¤å’Œæ’åºæ©ç 
        valid_masks = [
            mask for mask in sorted(masks, key=(lambda x: x['area']), reverse=True) 
            if mask['area'] > 500 and mask['predicted_iou'] > 0.9
        ]  # æœ€å¤šå–8ä¸ªæœ€é‡è¦çš„åŒºåŸŸ
        
        # æå–åˆ†å‰²åŒºåŸŸ
        patches = []
        for mask in valid_masks:
            # å°†æ©ç ç¼©æ”¾åˆ°åŸå§‹å°ºå¯¸
            mask_array = cv2.resize(
                mask["segmentation"].astype(np.uint8),
                (original_size[1], original_size[0]),
                interpolation=cv2.INTER_NEAREST
            )
            
            # æå–æ©ç åŒºåŸŸ
            masked = cv2.bitwise_and(
                cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR),
                cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR),
                mask=mask_array
            )
            
            # è½¬æ¢ä¸ºPIL Imageå¹¶è£å‰ªæœ‰æ•ˆåŒºåŸŸ
            y, x = np.where(mask_array)
            if len(x) == 0 or len(y) == 0:
                continue
            x_min, x_max = np.min(x), np.max(x)
            y_min, y_max = np.min(y), np.max(y)
            
            patch = Image.fromarray(cv2.cvtColor(masked[y_min:y_max+1, x_min:x_max+1], cv2.COLOR_BGR2RGB))
            patches.append(patch)
        
        return patches

    
    def _encode_images(self, patches: List[Image.Image]) -> torch.Tensor:
        """æ‰¹é‡å›¾åƒç¼–ç """
        if not all(isinstance(p, Image.Image) for p in patches):
            raise ValueError("All patches must be PIL.Image instances")
        with torch.no_grad():
            image_inputs = torch.stack([self.preprocess(patch) for patch in patches]).to(self.device)
            return self.model.encode_image(image_inputs)
    
    def _find_best_matches(
        self, 
        image_embeds: torch.Tensor, 
        text_embeds: torch.Tensor
    ) -> Tuple[np.ndarray, torch.Tensor]:  # ä¿®æ”¹è¿”å›å€¼ç±»å‹
        """
        è¿”å›ï¼š
        - matches: åŒ¹é…çš„ç´¢å¼•æ•°ç»„ï¼ˆæ— æ•ˆåŒ¹é…æ ‡è®°ä¸º -1ï¼‰
        - similarity_matrix: å®Œæ•´çš„ç›¸ä¼¼åº¦çŸ©é˜µ
        """
        # å½’ä¸€åŒ–åµŒå…¥å‘é‡
        image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)
        text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µï¼ˆæœªç¼©æ”¾ï¼‰
        similarity_matrix = image_embeds @ text_embeds.T
        
        # åº”ç”¨ CLIP çš„æ¸©åº¦ç¼©æ”¾
        logit_scale = self.model.logit_scale.exp()
        scaled_similarity = similarity_matrix * logit_scale
        
        # åŒ¹é…é€»è¾‘ï¼ˆåŸºäºç¼©æ”¾åçš„ç›¸ä¼¼åº¦ï¼‰
        max_sim = scaled_similarity.max(dim=1).values
        valid_mask = max_sim > self.similarity_threshold
        
        # ç”ŸæˆåŒ¹é…ç»“æœ
        matches = torch.full(
            (scaled_similarity.size(0),), 
            -1, 
            dtype=torch.long, 
            device=image_embeds.device
        )
        valid_indices = torch.where(valid_mask)[0]
        if valid_indices.numel() > 0:
            matches[valid_indices] = torch.argmax(scaled_similarity[valid_indices], dim=1)
        
        # è¿”å›åŒ¹é…ç´¢å¼•å’ŒåŸå§‹ç›¸ä¼¼åº¦çŸ©é˜µï¼ˆæœªç¼©æ”¾ï¼‰
        return matches.cpu().numpy(), similarity_matrix.cpu()
    
    def process_image(
        self, 
        image: Image.Image, 
        class_name: str, 
        save_patches: bool = False
    ) -> Dict:
        """å¤„ç†å•å¼ å›¾åƒå¹¶åº”ç”¨é˜ˆå€¼è¿‡æ»¤"""
        if class_name not in self.text_cache:
            raise ValueError(f"Class {class_name} not processed")
        
        # å›¾åƒåˆ†å‰²
        patches = self._segment_image(image)
        if not patches:
            return {}
        
        # ç¼–ç å›¾åƒå—
        image_embeds = self._encode_images(patches)
        
        # è·å–æ–‡æœ¬å±æ€§åµŒå…¥
        text_data = self.text_cache[class_name]
        if not text_data["attributes"]:
            return {}
        
        attribute_tensor = torch.stack(text_data["attributes"]).to(self.device)
        
        # è·å–åŒ¹é…ç»“æœå’Œç›¸ä¼¼åº¦çŸ©é˜µ
        matches, similarity_matrix = self._find_best_matches(image_embeds, attribute_tensor)
        
        # æ„å»ºç»“æœï¼ˆåº”ç”¨é˜ˆå€¼è¿‡æ»¤ï¼‰
        results = defaultdict(list)
        for idx, match_idx in enumerate(matches):
            if match_idx == -1:  # è·³è¿‡æ— æ•ˆåŒ¹é…
                continue
            
            attr_name = text_data["attribute_names"][match_idx]
            similarity = similarity_matrix[idx, match_idx].item()  # ä½¿ç”¨åŸå§‹ç›¸ä¼¼åº¦
            
            if similarity >= self.similarity_threshold:
                results[attr_name].append({
                    "embedding": image_embeds[idx].cpu().clone(),
                    "patch": patches[idx] if save_patches else None,
                    "similarity": similarity  # å­˜å‚¨ç›¸ä¼¼åº¦å€¼
                })
        
        # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
        for attr in results:
            results[attr].sort(key=lambda x: x["similarity"], reverse=True)
        
        return results
    
    def add_to_knowledge(self, class_name: str, image_results: Dict):
        """ä¿®æ­£åçš„æ·»åŠ æ–¹æ³•ï¼Œå­˜å‚¨å®Œæ•´å­—å…¸ç»“æ„"""
        if class_name not in self.text_cache:
            raise ValueError(f"Class {class_name} not initialized")
        
        for attr_name, items in image_results.items():
            # è¿™é‡Œæ”¹ä¸ºå­˜å‚¨å®Œæ•´å­—å…¸è€Œéä»…embedding
            self.knowledge[class_name]["image_embeds"][attr_name].extend(items)
        
        # æ›´æ–°ç´¢å¼•ç¼“å­˜
        if class_name in self.index_cache:
            del self.index_cache[class_name]
    
    def _build_index(self, class_name: str):
        """ä¿®æ­£åçš„ç´¢å¼•æ„å»ºæ–¹æ³•"""
        all_embeds = []
        mapping = []
        
        for attr, items in self.knowledge[class_name]["image_embeds"].items():
            for idx, item in enumerate(items):
                all_embeds.append(item["embedding"].numpy())
                mapping.append((attr, idx))
        
        self.index_cache[class_name] = {
            "embeddings": np.stack(all_embeds),
            "mapping": mapping
        }
    
    def query(self, 
            query_embed: torch.Tensor, 
            class_name: str, 
            top_k: int = 5) -> List[Tuple[str, float, int]]:
        """ä¿®æ­£åçš„æŸ¥è¯¢æ–¹æ³•"""
        if class_name not in self.index_cache:
            self._build_index(class_name)
        
        index_data = self.index_cache[class_name]
        query_np = query_embed.cpu().numpy()
        
        # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
        norms = np.linalg.norm(index_data["embeddings"], axis=1)
        query_norm = np.linalg.norm(query_np)
        similarities = (index_data["embeddings"] @ query_np) / (norms * query_norm)
        
        top_indices = np.argsort(-similarities)[:top_k]
        
        return [(
            index_data["mapping"][i][0],  # å±æ€§å
            float(similarities[i]),       # ç›¸ä¼¼åº¦
            index_data["mapping"][i][1]   # åŸå§‹ç´¢å¼•
        ) for i in top_indices]
    
    def save(self, filename: str):
        """ä¿å­˜çŸ¥è¯†åº“"""
        knowledge_data = {
            "knowledge": dict(self.knowledge),
            "text_cache": self.text_cache
        }
        joblib.dump(knowledge_data, filename)
    
    @classmethod
    def load(cls, filename: str, device: str = None):
        """åŠ è½½çŸ¥è¯†åº“"""
        self = cls(device=device)
        data = joblib.load(filename)
        self.knowledge = defaultdict(lambda: {
            "text_embeds": None,
            "image_embeds": defaultdict(list)
        }, data["knowledge"])
        self.text_cache = data["text_cache"]
        return self

    def get_class_data(self, class_name: str) -> Dict:
        """è·å–æŒ‡å®šç±»åˆ«çš„å®Œæ•´æ•°æ®"""
        return {
            "text_embeds": self.text_cache[class_name],
            "image_embeds": dict(self.knowledge[class_name]["image_embeds"])
        }
    
    def _wrap_text(self, text: str, max_length: int = 25) -> str:
        """è‡ªåŠ¨æ¢è¡Œæ–‡æœ¬å¤„ç†"""
        words = text.split()
        lines = []
        current_line = []
        
        for word in words:
            if len(' '.join(current_line + [word])) <= max_length:
                current_line.append(word)
            else:
                lines.append(' '.join(current_line))
                current_line = [word]
        lines.append(' '.join(current_line))
        
        return '\n'.join(lines)
    
    def print_structure(self):
        """æ‰“å°çŸ¥è¯†åº“å®Œæ•´ç»“æ„"""
        print("="*60 + "\nKnowledge Base Structure\n" + "="*60)
        
        # æ‰“å°æ–‡æœ¬ç¼“å­˜
        print("\n[Text Cache]")
        for cls, data in self.text_cache.items():
            print(f"Class: {cls}")
            print(f"â”œâ”€ Overall Embedding: {data['overall'].shape}")
            print(f"â”œâ”€ Class Embedding: {data['class'].shape}")
            print(f"â””â”€ Attributes ({len(data['attribute_names'])}):")
            for name, emb in zip(data['attribute_names'], data['attributes']):
                print(f"   â”œâ”€ Embedding of {name}: {emb.shape}")

        # æ‰“å°å›¾åƒçŸ¥è¯†
        print("\n[Image Knowledge]")
        for cls, data in self.knowledge.items():
            print(f"Class: {cls}")
            total = sum(len(v) for v in data['image_embeds'].values())
            print(f"â””â”€ Total Image Embeds: {total}")
            for attr, items in data['image_embeds'].items():
                print(f"   â”œâ”€ {attr} ({len(items)} items)")
                if items:
                    sample = items[0]
                    print(f"   â”‚  â”œâ”€ Embedding: {sample['embedding'].shape}")
                    print(f"   â”‚  â”œâ”€ Image Patch: {sample['patch'].size}")
                    print(f"   â”‚  â””â”€ Has Patch: {'patch' in sample}")

        # æ‰“å°ç´¢å¼•ç¼“å­˜
        print("\n[Index Cache]")
        for cls, data in self.index_cache.items():
            print(f"Class: {cls}")
            print(f"â”œâ”€ Embeddings Shape: {data['embeddings'].shape}")
            print(f"â””â”€ Mapping Count: {len(data['mapping'])}")

if __name__ == "__main__":
    import matplotlib.pyplot as plt
    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = "/data/lihl/fsod/model/knowledge_base/result"
    os.makedirs(save_dir, exist_ok=True)
    # åˆå§‹åŒ–çŸ¥è¯†åº“
    device = "cuda" if torch.cuda.is_available() else "cpu"
    kb = Knowledge_Base(sam_model_type="vit_h")

    # ç¤ºä¾‹1ï¼šå¤„ç†é©¬ç±»çš„å±æ€§æ–‡æœ¬
    horse_attributes = "Horses: a muscular build, a long mane, a flowing tail, slender legs, a large head, expressive eyes, pointed ears, and hard hooves."
    kb.process_attributes(horse_attributes)

    # ç¤ºä¾‹2ï¼šå¤„ç†é¸Ÿç±»çš„å±æ€§æ–‡æœ¬
    bird_attributes = "Birds: a light skeleton, feathers covering the body, a beak, two wings, hollow bones, a streamlined body shape, and webbed feet."
    kb.process_attributes(bird_attributes)

    # å¤„ç†é©¬ç±»å›¾åƒç¤ºä¾‹
    def process_horse_image(image_path):
        # åŠ è½½å¹¶å¤„ç†å›¾åƒ
        img = Image.open(image_path).convert("RGB")
        results = kb.process_image(img, "Horses", save_patches=True)
    
        patch_info = []
        for attr_name, items in results.items():
            for item in items:
                if item["patch"] is not None:
                    patch_info.append( (item["patch"], attr_name) )
        
        # å¯è§†åŒ–åˆ†å‰²ç»“æœï¼ˆå¸¦å±æ€§åç§°æ ‡ç­¾ï¼‰
        fig = plt.figure(figsize=(16, 8))
        plt.suptitle("SAM Segmented Patches with Attributes", fontsize=14, y=0.95)
        
        cols = 4
        rows = (len(patch_info) + cols - 1) // cols
        
        for i, (patch, attr) in enumerate(patch_info, 1):
            ax = plt.subplot(rows, cols, i)
            ax.imshow(patch)
            ax.axis('off')
            
            # è‡ªåŠ¨æ¢è¡Œå¤„ç†é•¿æ–‡æœ¬
            wrapped_text = kb._wrap_text(attr, max_length=25)
            ax.set_title(wrapped_text, 
                        fontsize=9, 
                        pad=6,
                        color='#2c3e50',
                        fontweight='semibold',
                        loc='center',
                        wrap=True)
        
        plt.tight_layout(pad=2.0, h_pad=1.0, w_pad=1.0)
        
        # ä¿å­˜åˆ†å‰²ç»“æœ
        timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
        save_path = os.path.join("/data/lihl/fsod/model/knowledge_base/result", 
                                f"sam_segments_{timestamp}.png")
        fig.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close(fig)
        print(f"âœ… Saved SAM segments to {save_path}")
        
        # æ·»åŠ åˆ°çŸ¥è¯†åº“
        kb.add_to_knowledge("Horses", results)
        added_count = sum(len(v) for v in results.values())
        print(f"ğŸ“¥ Added {added_count} embeddings to Horses knowledge")

    # å¤„ç†å¤šä¸ªç¤ºä¾‹å›¾åƒ
    process_horse_image("/data2/lihl/data/VOCdevkit/VOC2007/JPEGImages/000214.jpg")  # æ›¿æ¢ä¸ºå®é™…å›¾ç‰‡è·¯å¾„
    process_horse_image("/data2/lihl/data/VOCdevkit/VOC2007/JPEGImages/000328.jpg")

    # ä¿å­˜çŸ¥è¯†åº“
    kb.save("/data/lihl/fsod/model/knowledge_base/animal_knowledge.pkl")
    
    # åŠ è½½çŸ¥è¯†åº“ï¼ˆå¯é€‰ï¼‰
    # kb = Knowledge_Base.load("animal_knowledge.pkl", device=device)

    def query_example(query_image_path):
        # åŠ è½½å¹¶åˆ†å‰²æŸ¥è¯¢å›¾åƒ
        query_img = Image.open(query_image_path).convert("RGB")
        patches = kb._segment_image(query_img)
        print(len(patches))
        print(patches.shape)
        for index in range(len(patches)):
            # é€‰æ‹©ç¬¬ä¸€ä¸ªpatchä½œä¸ºæŸ¥è¯¢ç¤ºä¾‹
            query_embed = kb._encode_images([patches[index]]).squeeze(0)
            
            # åœ¨çŸ¥è¯†åº“ä¸­æœç´¢
            results = kb.query(query_embed, "Horses", top_k=3)
            
            # å¯è§†åŒ–æŸ¥è¯¢ç»“æœ
            fig = plt.figure(figsize=(15, 5))
            plt.subplot(1,4,1)
            plt.title("Query Patch")
            plt.imshow(patches[0])
            plt.axis('off')
            
            for i, (attr_name, similarity, idx) in enumerate(results):
                plt.subplot(1,4,i+2)
                plt.title(f"{attr_name}\nSimilarity: {similarity:.2f}")
                # ä¿®æ­£è®¿é—®æ–¹å¼
                example_item = kb.knowledge["Horses"]["image_embeds"][attr_name][idx]
                if example_item.get("patch"):
                    plt.imshow(example_item["patch"])
                plt.axis('off')
            plt.tight_layout()
            timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
            save_path = os.path.join(save_dir, f"query_{index}.png")
            fig.savefig(save_path)
            plt.close(fig)
            print(f"Saved query result to {save_path}")
    kb.print_structure()

    # æ‰§è¡ŒæŸ¥è¯¢ï¼ˆä½¿ç”¨æ–°å›¾åƒï¼‰
    query_example("/data2/lihl/data/VOCdevkit/VOC2007/JPEGImages/000332.jpg")  # æ›¿æ¢ä¸ºå®é™…æŸ¥è¯¢å›¾ç‰‡è·¯å¾„